{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciffar-Images-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = unpickle(r\"C:\\Users\\siava\\OneDrive - University of Manitoba\\D\\Computer science\\Data Science Projects\\Ciffar-Image-Classification\\Data\\data_batch_1\")\n",
    "data_2 = unpickle(r\"C:\\Users\\siava\\OneDrive - University of Manitoba\\D\\Computer science\\Data Science Projects\\Ciffar-Image-Classification\\Data\\data_batch_2\")\n",
    "data_3 = unpickle(r\"C:\\Users\\siava\\OneDrive - University of Manitoba\\D\\Computer science\\Data Science Projects\\Ciffar-Image-Classification\\Data\\data_batch_3\")\n",
    "data_4 = unpickle(r\"C:\\Users\\siava\\OneDrive - University of Manitoba\\D\\Computer science\\Data Science Projects\\Ciffar-Image-Classification\\Data\\data_batch_4\")\n",
    "data_5 = unpickle(r\"C:\\Users\\siava\\OneDrive - University of Manitoba\\D\\Computer science\\Data Science Projects\\Ciffar-Image-Classification\\Data\\data_batch_5\")\n",
    "test_batch = unpickle(r\"C:\\Users\\siava\\OneDrive - University of Manitoba\\D\\Computer science\\Data Science Projects\\Ciffar-Image-Classification\\Data\\test_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[b\"data\"] = data_1[b\"data\"].astype(float)\n",
    "data_2[b\"data\"] = data_2[b\"data\"].astype(float)\n",
    "data_3[b\"data\"] = data_3[b\"data\"].astype(float)\n",
    "data_4[b\"data\"] = data_4[b\"data\"].astype(float)\n",
    "data_5[b\"data\"] = data_5[b\"data\"].astype(float)\n",
    "test_batch[b\"data\"] = test_batch[b\"data\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistanceCalculation(training_data, validation_data):\n",
    "    distances = np.zeros((40000, 10000))\n",
    "  \n",
    "    #Calculating Euclidean distance for every validation image\n",
    "    for i in range(0,10000):\n",
    "    \n",
    "        for j in range(0,40000):\n",
    "            \n",
    "            a = validation_data[i] - training_data[j]\n",
    "            distances[j,i] = distances[j,i] + np.sqrt(np.dot(a.T,a))\n",
    "            print(f\"({j},{i})\")\n",
    "\n",
    "    np.savetxt(\"distances.csv\", distances, delimiter=\",\")\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistanceCalculationWithNormalization(training_data, validation_data):\n",
    "    norm_distances = np.zeros((40000, 10000))\n",
    "  \n",
    "  #Calculating Euclidean distance for every validation image with normalization\n",
    "    for i in range(0,10000):\n",
    "        val_mean = validation_data[i].mean()\n",
    "        val_std = validation_data[i].std()\n",
    "        normalized_val = (validation_data[i] - val_mean)/val_std\n",
    "    \n",
    "        for j in range(0,40000):\n",
    "            train_mean = training_data[j].mean()\n",
    "            train_std = training_data[j].std()\n",
    "            normalized_train = (training_data[j] - train_mean)/train_std\n",
    "            a = normalized_val - normalized_train\n",
    "            norm_distances[j,i] = norm_distances[j,i] + np.sqrt(np.dot(a.T,a))\n",
    "            print(f\"({j},{i})\")\n",
    "            \n",
    "    np.savetxt(\"Normdistances.csv\", norm_distances, delimiter=\",\")\n",
    "    return norm_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KfoldValidation(distances,training_lables, validation_labels,k):\n",
    "  \n",
    "    #sorting the distances \n",
    "    sorted_distances = np.sort(distances, axis=1)\n",
    "\n",
    "    #finding the indices of k smallest distances\n",
    "    sorted_distances_indices = np.argsort(sorted_distances, axis=0)\n",
    "    Ksmallest_distances_indices = sorted_distances_indices[0:k, ]\n",
    " \n",
    "    #Finding the labels of K smallest distances using their indices\n",
    "    Ksmallest_labels = np.zeros((k,10000))\n",
    "    for i in range(0,10000):\n",
    "        for j in range(0,k):\n",
    "            Ksmallest_labels[j,i] = Ksmallest_labels[j,i] + training_lables[Ksmallest_distances_indices[j,i]]\n",
    "  \n",
    "  \n",
    "  #predicted label: most frequent label in k smallest distances'label\n",
    "  #argmax(array, axis = None, out = None) : Returns indices of the max element of the array in a particular axis\n",
    "  #bincount: Count number of occurrences of each value in array of non-negative ints.\n",
    "    predicted_labels = []\n",
    "    for i in range(0,10000):\n",
    "        a = Ksmallest_labels[:,i].astype(int)\n",
    "        predicted_labels.append(np.bincount(a).argmax())\n",
    "  \n",
    "\n",
    "  #comparing our result and calculating accuracy\n",
    "    a = 0\n",
    "    for i in range(0,10000):\n",
    "        if predicted_labels[i] == validation_labels[i]:\n",
    "            a = a + 1\n",
    "  \n",
    "  accuracy = a/10000\n",
    "  \n",
    "  return (accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1 = np.concatenate((data_1[b\"data\"],data_2[b\"data\"],data_3[b\"data\"],data_4[b\"data\"]), axis=0)\n",
    "training_labels_1 = data_1[b\"labels\"] + data_2[b\"labels\"] + data_3[b\"labels\"] + data_4[b\"labels\"] \n",
    "validation_data_1 = data_5[b\"data\"]\n",
    "validation_labels_1 = data_5[b\"labels\"]\n",
    "\n",
    "training_data_2 = np.concatenate((data_1[b\"data\"],data_2[b\"data\"],data_3[b\"data\"],data_5[b\"data\"]), axis=0)\n",
    "training_labels_2 = data_1[b\"labels\"] + data_2[b\"labels\"] + data_3[b\"labels\"] + data_5[b\"labels\"] \n",
    "validation_data_2 = data_4[b\"data\"]\n",
    "validation_labels_2 = data_4[b\"labels\"]\n",
    "\n",
    "\n",
    "training_data_3 = np.concatenate((data_1[b\"data\"],data_2[b\"data\"],data_5[b\"data\"],data_4[b\"data\"]), axis=0)\n",
    "training_labels_3 = data_1[b\"labels\"] + data_2[b\"labels\"] + data_5[b\"labels\"] + data_4[b\"labels\"] \n",
    "validation_data_2 = data_3[b\"data\"]\n",
    "validation_labels_2 = data_3[b\"labels\"]\n",
    "\n",
    "\n",
    "training_data_4 = np.concatenate((data_1[b\"data\"],data_5[b\"data\"],data_3[b\"data\"],data_4[b\"data\"]), axis=0)\n",
    "training_labels_4 = data_1[b\"labels\"] + data_5[b\"labels\"] + data_3[b\"labels\"] + data_4[b\"labels\"] \n",
    "validation_data_2 = data_2[b\"data\"]\n",
    "validation_labels_2 = data_2[b\"labels\"]\n",
    "\n",
    "\n",
    "training_data_5 = np.concatenate((data_5[b\"data\"],data_2[b\"data\"],data_3[b\"data\"],data_4[b\"data\"]), axis=0)\n",
    "training_labels_5 = data_5[b\"labels\"] + data_2[b\"labels\"] + data_3[b\"labels\"] + data_4[b\"labels\"] \n",
    "validation_data_2 = data_1[b\"data\"]\n",
    "validation_labels_2 = data_1[b\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_final = np.concatenate((data_1[b\"data\"],data_2[b\"data\"],data_3[b\"data\"],data_4[b\"data\"],data_5[b\"data\"]), axis=0)\n",
    "training_labels_final = data_1[b\"labels\"] + data_2[b\"labels\"] + data_3[b\"labels\"] + data_4[b\"labels\"] + data_5[b\"labels\"]\n",
    "test_data = test_batch[b\"data\"]\n",
    "test_labels = test_batch[b\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_1 = DistanceCalculation(training_data_1, validation_data_1)\n",
    "distance_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
